{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60fff65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288d12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import AutoROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4fe219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Beam Rider environment loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"BeamRiderNoFrameskip-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "print(\"✅ Beam Rider environment loaded successfully!\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5912441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     obs, info = env.reset()\n",
    "#     done = False\n",
    "\n",
    "#     print(f\"Episode {i+1}:\")\n",
    "\n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         obs, reward, terminated, truncated, info = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82910b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super().__init__()\n",
    "        c, h, w = obs_shape  # now c = 4\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(obs_shape)\n",
    "        self.fc = nn.Sequential(nn.Linear(conv_out_size, 512), nn.ReLU())\n",
    "        self.actor = nn.Linear(512, n_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        o = self.conv(o)\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0  # normalize pixel values\n",
    "        conv_out = self.conv(x).view(x.size(0), -1)\n",
    "        fc_out = self.fc(conv_out)\n",
    "        return self.actor(fc_out), self.critic(fc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de30029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, obs_shape, n_actions, lr=2.5e-4, gamma=0.99, clip_eps=0.1, k_epochs=4):\n",
    "        self.gamma = gamma\n",
    "        self.clip_eps = clip_eps\n",
    "        self.k_epochs = k_epochs\n",
    "\n",
    "        self.policy = ActorCritic(obs_shape, n_actions)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # (1, 4, 84, 84)\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.policy(state)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), value.item()\n",
    "\n",
    "    def compute_returns(self, rewards, dones, next_value):\n",
    "        R = next_value\n",
    "        returns = []\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            R = r + self.gamma * R * (1 - d)\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    def update(self, memory):\n",
    "        states = torch.FloatTensor(np.array(memory[\"states\"]))\n",
    "        actions = torch.LongTensor(memory[\"actions\"])\n",
    "        old_log_probs = torch.stack(memory[\"log_probs\"]).detach()\n",
    "        returns = torch.FloatTensor(memory[\"returns\"])\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            logits, values = self.policy(states)\n",
    "            dist = Categorical(torch.softmax(logits, dim=-1))\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratios = (log_probs - old_log_probs).exp()\n",
    "            advantages = returns - values.squeeze().detach()\n",
    "\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_eps, 1 + self.clip_eps) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ef136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    # Convert RGB (210x160x3) to grayscale + resize (84x84)\n",
    "    import cv2\n",
    "    obs_gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "    obs_resized = cv2.resize(obs_gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return obs_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41c3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_beamrider(episodes=500, log_interval=10, save_path=\"beamrider_ppo.pth\"):\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_dir = os.path.join(\"runs\", f\"beamrider_{run_id}\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file_path = os.path.join(log_dir, \"training_log.csv\")\n",
    "\n",
    "    # Initialize CSV log file\n",
    "    if not os.path.exists(log_file_path):\n",
    "        with open(log_file_path, \"w\") as f:\n",
    "            f.write(\"episode,total_reward,avg_reward_10,best_reward\\n\")\n",
    "\n",
    "    # Use the working legacy name (not ALE/)\n",
    "    env = gym.make(\"BeamRiderNoFrameskip-v4\", render_mode=\"human\")\n",
    "    obs, _ = env.reset()\n",
    "    obs = preprocess(obs)\n",
    "    state_shape = (4, 84, 84)  # 4 stacked frames\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    agent = PPOAgent(state_shape, n_actions)\n",
    "    reward_history = []\n",
    "    best_avg_reward = -float(\"inf\")\n",
    "\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(np.zeros((84, 84), dtype=np.uint8))\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        frame_stack.extend([preprocess(obs)] * 4)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        memory = {\"states\": [], \"actions\": [], \"log_probs\": [], \"rewards\": [], \"dones\": [], \"returns\": []}\n",
    "\n",
    "        while not done:\n",
    "            state = np.stack(frame_stack, axis=0)\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            frame_stack.append(preprocess(obs))\n",
    "\n",
    "            memory[\"states\"].append(state)\n",
    "            memory[\"actions\"].append(action)\n",
    "            memory[\"log_probs\"].append(log_prob)\n",
    "            memory[\"rewards\"].append(reward)\n",
    "            memory[\"dones\"].append(done)\n",
    "            total_reward += reward\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = np.stack(frame_stack, axis=0)\n",
    "            _, next_value = agent.policy(torch.FloatTensor(state).unsqueeze(0))\n",
    "            next_value = next_value.item()\n",
    "\n",
    "        memory[\"returns\"] = agent.compute_returns(memory[\"rewards\"], memory[\"dones\"], next_value)\n",
    "        agent.update(memory)\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        # Compute recent average reward\n",
    "        recent_rewards = reward_history[-10:] if len(reward_history) >= 10 else reward_history\n",
    "        avg_r_10 = float(np.mean(recent_rewards))\n",
    "\n",
    "        # Track best model based on 10-episode average\n",
    "        is_best = avg_r_10 > best_avg_reward\n",
    "        if is_best:\n",
    "            best_avg_reward = avg_r_10\n",
    "            torch.save({\n",
    "                \"policy_state_dict\": agent.policy.state_dict(),\n",
    "                \"optimizer_state_dict\": agent.optimizer.state_dict(),\n",
    "                \"reward_history\": reward_history,\n",
    "                \"episode\": ep + 1,\n",
    "                \"best_avg_reward\": best_avg_reward\n",
    "            }, os.path.join(log_dir, \"best_model.pth\"))\n",
    "\n",
    "        # Append to CSV log\n",
    "        with open(log_file_path, \"a\") as f:\n",
    "            f.write(f\"{ep+1},{total_reward},{avg_r_10},{best_avg_reward}\\n\")\n",
    "\n",
    "        # Console logging\n",
    "        print(f\"Episode {ep+1}: Reward = {total_reward:.1f}, Avg10 = {avg_r_10:.2f}, BestAvg10 = {best_avg_reward:.2f}\")\n",
    "\n",
    "        # Periodic checkpoint saving\n",
    "        if (ep + 1) % log_interval == 0:\n",
    "            ckpt_path = os.path.join(log_dir, f\"checkpoint_ep{ep+1}.pth\")\n",
    "            torch.save({\n",
    "                \"policy_state_dict\": agent.policy.state_dict(),\n",
    "                \"optimizer_state_dict\": agent.optimizer.state_dict(),\n",
    "                \"reward_history\": reward_history,\n",
    "                \"episode\": ep + 1,\n",
    "                \"best_avg_reward\": best_avg_reward\n",
    "            }, ckpt_path)\n",
    "            print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "    env.close()\n",
    "    plt.plot(reward_history)\n",
    "    plt.title(\"PPO on BeamRider-v4\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1252a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 308.0, Avg10 = 308.00, BestAvg10 = 308.00\n",
      "Episode 2: Reward = 176.0, Avg10 = 242.00, BestAvg10 = 308.00\n",
      "Episode 2: Reward = 176.0, Avg10 = 242.00, BestAvg10 = 308.00\n",
      "Episode 3: Reward = 220.0, Avg10 = 234.67, BestAvg10 = 308.00\n",
      "Episode 3: Reward = 220.0, Avg10 = 234.67, BestAvg10 = 308.00\n",
      "Episode 4: Reward = 308.0, Avg10 = 253.00, BestAvg10 = 308.00\n",
      "Episode 4: Reward = 308.0, Avg10 = 253.00, BestAvg10 = 308.00\n",
      "Episode 5: Reward = 264.0, Avg10 = 255.20, BestAvg10 = 308.00\n",
      "Episode 5: Reward = 264.0, Avg10 = 255.20, BestAvg10 = 308.00\n",
      "Episode 6: Reward = 308.0, Avg10 = 264.00, BestAvg10 = 308.00\n",
      "Episode 6: Reward = 308.0, Avg10 = 264.00, BestAvg10 = 308.00\n",
      "Episode 7: Reward = 132.0, Avg10 = 245.14, BestAvg10 = 308.00\n",
      "Episode 7: Reward = 132.0, Avg10 = 245.14, BestAvg10 = 308.00\n",
      "Episode 8: Reward = 440.0, Avg10 = 269.50, BestAvg10 = 308.00\n",
      "Episode 8: Reward = 440.0, Avg10 = 269.50, BestAvg10 = 308.00\n",
      "Episode 9: Reward = 484.0, Avg10 = 293.33, BestAvg10 = 308.00\n",
      "Episode 9: Reward = 484.0, Avg10 = 293.33, BestAvg10 = 308.00\n",
      "Episode 10: Reward = 352.0, Avg10 = 299.20, BestAvg10 = 308.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep10.pth\n",
      "Episode 10: Reward = 352.0, Avg10 = 299.20, BestAvg10 = 308.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep10.pth\n",
      "Episode 11: Reward = 132.0, Avg10 = 281.60, BestAvg10 = 308.00\n",
      "Episode 11: Reward = 132.0, Avg10 = 281.60, BestAvg10 = 308.00\n",
      "Episode 12: Reward = 220.0, Avg10 = 286.00, BestAvg10 = 308.00\n",
      "Episode 12: Reward = 220.0, Avg10 = 286.00, BestAvg10 = 308.00\n",
      "Episode 13: Reward = 572.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 13: Reward = 572.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 14: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 14: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 15: Reward = 264.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 15: Reward = 264.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 16: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 16: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 321.20\n",
      "Episode 17: Reward = 440.0, Avg10 = 352.00, BestAvg10 = 352.00\n",
      "Episode 17: Reward = 440.0, Avg10 = 352.00, BestAvg10 = 352.00\n",
      "Episode 18: Reward = 176.0, Avg10 = 325.60, BestAvg10 = 352.00\n",
      "Episode 18: Reward = 176.0, Avg10 = 325.60, BestAvg10 = 352.00\n",
      "Episode 19: Reward = 220.0, Avg10 = 299.20, BestAvg10 = 352.00\n",
      "Episode 19: Reward = 220.0, Avg10 = 299.20, BestAvg10 = 352.00\n",
      "Episode 20: Reward = 352.0, Avg10 = 299.20, BestAvg10 = 352.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep20.pth\n",
      "Episode 20: Reward = 352.0, Avg10 = 299.20, BestAvg10 = 352.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep20.pth\n",
      "Episode 21: Reward = 264.0, Avg10 = 312.40, BestAvg10 = 352.00\n",
      "Episode 21: Reward = 264.0, Avg10 = 312.40, BestAvg10 = 352.00\n",
      "Episode 22: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 22: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 23: Reward = 572.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 23: Reward = 572.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 24: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 24: Reward = 308.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 25: Reward = 176.0, Avg10 = 312.40, BestAvg10 = 352.00\n",
      "Episode 25: Reward = 176.0, Avg10 = 312.40, BestAvg10 = 352.00\n",
      "Episode 26: Reward = 132.0, Avg10 = 294.80, BestAvg10 = 352.00\n",
      "Episode 26: Reward = 132.0, Avg10 = 294.80, BestAvg10 = 352.00\n",
      "Episode 27: Reward = 352.0, Avg10 = 286.00, BestAvg10 = 352.00\n",
      "Episode 27: Reward = 352.0, Avg10 = 286.00, BestAvg10 = 352.00\n",
      "Episode 28: Reward = 528.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 28: Reward = 528.0, Avg10 = 321.20, BestAvg10 = 352.00\n",
      "Episode 29: Reward = 484.0, Avg10 = 347.60, BestAvg10 = 352.00\n",
      "Episode 29: Reward = 484.0, Avg10 = 347.60, BestAvg10 = 352.00\n",
      "Episode 30: Reward = 660.0, Avg10 = 378.40, BestAvg10 = 378.40\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep30.pth\n",
      "Episode 30: Reward = 660.0, Avg10 = 378.40, BestAvg10 = 378.40\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep30.pth\n",
      "Episode 31: Reward = 220.0, Avg10 = 374.00, BestAvg10 = 378.40\n",
      "Episode 31: Reward = 220.0, Avg10 = 374.00, BestAvg10 = 378.40\n",
      "Episode 32: Reward = 176.0, Avg10 = 360.80, BestAvg10 = 378.40\n",
      "Episode 32: Reward = 176.0, Avg10 = 360.80, BestAvg10 = 378.40\n",
      "Episode 33: Reward = 396.0, Avg10 = 343.20, BestAvg10 = 378.40\n",
      "Episode 33: Reward = 396.0, Avg10 = 343.20, BestAvg10 = 378.40\n",
      "Episode 34: Reward = 88.0, Avg10 = 321.20, BestAvg10 = 378.40\n",
      "Episode 34: Reward = 88.0, Avg10 = 321.20, BestAvg10 = 378.40\n",
      "Episode 35: Reward = 484.0, Avg10 = 352.00, BestAvg10 = 378.40\n",
      "Episode 35: Reward = 484.0, Avg10 = 352.00, BestAvg10 = 378.40\n",
      "Episode 36: Reward = 528.0, Avg10 = 391.60, BestAvg10 = 391.60\n",
      "Episode 36: Reward = 528.0, Avg10 = 391.60, BestAvg10 = 391.60\n",
      "Episode 37: Reward = 220.0, Avg10 = 378.40, BestAvg10 = 391.60\n",
      "Episode 37: Reward = 220.0, Avg10 = 378.40, BestAvg10 = 391.60\n",
      "Episode 38: Reward = 308.0, Avg10 = 356.40, BestAvg10 = 391.60\n",
      "Episode 38: Reward = 308.0, Avg10 = 356.40, BestAvg10 = 391.60\n",
      "Episode 39: Reward = 176.0, Avg10 = 325.60, BestAvg10 = 391.60\n",
      "Episode 39: Reward = 176.0, Avg10 = 325.60, BestAvg10 = 391.60\n",
      "Episode 40: Reward = 440.0, Avg10 = 303.60, BestAvg10 = 391.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep40.pth\n",
      "Episode 40: Reward = 440.0, Avg10 = 303.60, BestAvg10 = 391.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep40.pth\n",
      "Episode 41: Reward = 616.0, Avg10 = 343.20, BestAvg10 = 391.60\n",
      "Episode 41: Reward = 616.0, Avg10 = 343.20, BestAvg10 = 391.60\n",
      "Episode 42: Reward = 396.0, Avg10 = 365.20, BestAvg10 = 391.60\n",
      "Episode 42: Reward = 396.0, Avg10 = 365.20, BestAvg10 = 391.60\n",
      "Episode 43: Reward = 88.0, Avg10 = 334.40, BestAvg10 = 391.60\n",
      "Episode 43: Reward = 88.0, Avg10 = 334.40, BestAvg10 = 391.60\n",
      "Episode 44: Reward = 528.0, Avg10 = 378.40, BestAvg10 = 391.60\n",
      "Episode 44: Reward = 528.0, Avg10 = 378.40, BestAvg10 = 391.60\n",
      "Episode 45: Reward = 264.0, Avg10 = 356.40, BestAvg10 = 391.60\n",
      "Episode 45: Reward = 264.0, Avg10 = 356.40, BestAvg10 = 391.60\n",
      "Episode 46: Reward = 264.0, Avg10 = 330.00, BestAvg10 = 391.60\n",
      "Episode 46: Reward = 264.0, Avg10 = 330.00, BestAvg10 = 391.60\n",
      "Episode 47: Reward = 264.0, Avg10 = 334.40, BestAvg10 = 391.60\n",
      "Episode 47: Reward = 264.0, Avg10 = 334.40, BestAvg10 = 391.60\n",
      "Episode 48: Reward = 176.0, Avg10 = 321.20, BestAvg10 = 391.60\n",
      "Episode 48: Reward = 176.0, Avg10 = 321.20, BestAvg10 = 391.60\n",
      "Episode 49: Reward = 484.0, Avg10 = 352.00, BestAvg10 = 391.60\n",
      "Episode 49: Reward = 484.0, Avg10 = 352.00, BestAvg10 = 391.60\n",
      "Episode 50: Reward = 264.0, Avg10 = 334.40, BestAvg10 = 391.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep50.pth\n",
      "Episode 50: Reward = 264.0, Avg10 = 334.40, BestAvg10 = 391.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep50.pth\n",
      "Episode 51: Reward = 484.0, Avg10 = 321.20, BestAvg10 = 391.60\n",
      "Episode 51: Reward = 484.0, Avg10 = 321.20, BestAvg10 = 391.60\n",
      "Episode 52: Reward = 396.0, Avg10 = 321.20, BestAvg10 = 391.60\n",
      "Episode 52: Reward = 396.0, Avg10 = 321.20, BestAvg10 = 391.60\n",
      "Episode 53: Reward = 440.0, Avg10 = 356.40, BestAvg10 = 391.60\n",
      "Episode 53: Reward = 440.0, Avg10 = 356.40, BestAvg10 = 391.60\n",
      "Episode 54: Reward = 660.0, Avg10 = 369.60, BestAvg10 = 391.60\n",
      "Episode 54: Reward = 660.0, Avg10 = 369.60, BestAvg10 = 391.60\n",
      "Episode 55: Reward = 396.0, Avg10 = 382.80, BestAvg10 = 391.60\n",
      "Episode 55: Reward = 396.0, Avg10 = 382.80, BestAvg10 = 391.60\n",
      "Episode 56: Reward = 660.0, Avg10 = 422.40, BestAvg10 = 422.40\n",
      "Episode 56: Reward = 660.0, Avg10 = 422.40, BestAvg10 = 422.40\n",
      "Episode 57: Reward = 308.0, Avg10 = 426.80, BestAvg10 = 426.80\n",
      "Episode 57: Reward = 308.0, Avg10 = 426.80, BestAvg10 = 426.80\n",
      "Episode 58: Reward = 396.0, Avg10 = 448.80, BestAvg10 = 448.80\n",
      "Episode 58: Reward = 396.0, Avg10 = 448.80, BestAvg10 = 448.80\n",
      "Episode 59: Reward = 572.0, Avg10 = 457.60, BestAvg10 = 457.60\n",
      "Episode 59: Reward = 572.0, Avg10 = 457.60, BestAvg10 = 457.60\n",
      "Episode 60: Reward = 484.0, Avg10 = 479.60, BestAvg10 = 479.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep60.pth\n",
      "Episode 60: Reward = 484.0, Avg10 = 479.60, BestAvg10 = 479.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep60.pth\n",
      "Episode 61: Reward = 308.0, Avg10 = 462.00, BestAvg10 = 479.60\n",
      "Episode 61: Reward = 308.0, Avg10 = 462.00, BestAvg10 = 479.60\n",
      "Episode 62: Reward = 440.0, Avg10 = 466.40, BestAvg10 = 479.60\n",
      "Episode 62: Reward = 440.0, Avg10 = 466.40, BestAvg10 = 479.60\n",
      "Episode 63: Reward = 308.0, Avg10 = 453.20, BestAvg10 = 479.60\n",
      "Episode 63: Reward = 308.0, Avg10 = 453.20, BestAvg10 = 479.60\n",
      "Episode 64: Reward = 264.0, Avg10 = 413.60, BestAvg10 = 479.60\n",
      "Episode 64: Reward = 264.0, Avg10 = 413.60, BestAvg10 = 479.60\n",
      "Episode 65: Reward = 440.0, Avg10 = 418.00, BestAvg10 = 479.60\n",
      "Episode 65: Reward = 440.0, Avg10 = 418.00, BestAvg10 = 479.60\n",
      "Episode 66: Reward = 572.0, Avg10 = 409.20, BestAvg10 = 479.60\n",
      "Episode 66: Reward = 572.0, Avg10 = 409.20, BestAvg10 = 479.60\n",
      "Episode 67: Reward = 528.0, Avg10 = 431.20, BestAvg10 = 479.60\n",
      "Episode 67: Reward = 528.0, Avg10 = 431.20, BestAvg10 = 479.60\n",
      "Episode 68: Reward = 396.0, Avg10 = 431.20, BestAvg10 = 479.60\n",
      "Episode 68: Reward = 396.0, Avg10 = 431.20, BestAvg10 = 479.60\n",
      "Episode 69: Reward = 220.0, Avg10 = 396.00, BestAvg10 = 479.60\n",
      "Episode 69: Reward = 220.0, Avg10 = 396.00, BestAvg10 = 479.60\n",
      "Episode 70: Reward = 616.0, Avg10 = 409.20, BestAvg10 = 479.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep70.pth\n",
      "Episode 70: Reward = 616.0, Avg10 = 409.20, BestAvg10 = 479.60\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep70.pth\n",
      "Episode 71: Reward = 440.0, Avg10 = 422.40, BestAvg10 = 479.60\n",
      "Episode 71: Reward = 440.0, Avg10 = 422.40, BestAvg10 = 479.60\n",
      "Episode 72: Reward = 484.0, Avg10 = 426.80, BestAvg10 = 479.60\n",
      "Episode 72: Reward = 484.0, Avg10 = 426.80, BestAvg10 = 479.60\n",
      "Episode 73: Reward = 756.0, Avg10 = 471.60, BestAvg10 = 479.60\n",
      "Episode 73: Reward = 756.0, Avg10 = 471.60, BestAvg10 = 479.60\n",
      "Episode 74: Reward = 660.0, Avg10 = 511.20, BestAvg10 = 511.20\n",
      "Episode 74: Reward = 660.0, Avg10 = 511.20, BestAvg10 = 511.20\n",
      "Episode 75: Reward = 616.0, Avg10 = 528.80, BestAvg10 = 528.80\n",
      "Episode 75: Reward = 616.0, Avg10 = 528.80, BestAvg10 = 528.80\n",
      "Episode 76: Reward = 396.0, Avg10 = 511.20, BestAvg10 = 528.80\n",
      "Episode 76: Reward = 396.0, Avg10 = 511.20, BestAvg10 = 528.80\n",
      "Episode 77: Reward = 352.0, Avg10 = 493.60, BestAvg10 = 528.80\n",
      "Episode 77: Reward = 352.0, Avg10 = 493.60, BestAvg10 = 528.80\n",
      "Episode 78: Reward = 572.0, Avg10 = 511.20, BestAvg10 = 528.80\n",
      "Episode 78: Reward = 572.0, Avg10 = 511.20, BestAvg10 = 528.80\n",
      "Episode 79: Reward = 528.0, Avg10 = 542.00, BestAvg10 = 542.00\n",
      "Episode 79: Reward = 528.0, Avg10 = 542.00, BestAvg10 = 542.00\n",
      "Episode 80: Reward = 308.0, Avg10 = 511.20, BestAvg10 = 542.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep80.pth\n",
      "Episode 80: Reward = 308.0, Avg10 = 511.20, BestAvg10 = 542.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep80.pth\n",
      "Episode 81: Reward = 220.0, Avg10 = 489.20, BestAvg10 = 542.00\n",
      "Episode 81: Reward = 220.0, Avg10 = 489.20, BestAvg10 = 542.00\n",
      "Episode 82: Reward = 616.0, Avg10 = 502.40, BestAvg10 = 542.00\n",
      "Episode 82: Reward = 616.0, Avg10 = 502.40, BestAvg10 = 542.00\n",
      "Episode 83: Reward = 264.0, Avg10 = 453.20, BestAvg10 = 542.00\n",
      "Episode 83: Reward = 264.0, Avg10 = 453.20, BestAvg10 = 542.00\n",
      "Episode 84: Reward = 176.0, Avg10 = 404.80, BestAvg10 = 542.00\n",
      "Episode 84: Reward = 176.0, Avg10 = 404.80, BestAvg10 = 542.00\n",
      "Episode 85: Reward = 396.0, Avg10 = 382.80, BestAvg10 = 542.00\n",
      "Episode 85: Reward = 396.0, Avg10 = 382.80, BestAvg10 = 542.00\n",
      "Episode 86: Reward = 616.0, Avg10 = 404.80, BestAvg10 = 542.00\n",
      "Episode 86: Reward = 616.0, Avg10 = 404.80, BestAvg10 = 542.00\n",
      "Episode 87: Reward = 176.0, Avg10 = 387.20, BestAvg10 = 542.00\n",
      "Episode 87: Reward = 176.0, Avg10 = 387.20, BestAvg10 = 542.00\n",
      "Episode 88: Reward = 660.0, Avg10 = 396.00, BestAvg10 = 542.00\n",
      "Episode 88: Reward = 660.0, Avg10 = 396.00, BestAvg10 = 542.00\n",
      "Episode 89: Reward = 308.0, Avg10 = 374.00, BestAvg10 = 542.00\n",
      "Episode 89: Reward = 308.0, Avg10 = 374.00, BestAvg10 = 542.00\n",
      "Episode 90: Reward = 352.0, Avg10 = 378.40, BestAvg10 = 542.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep90.pth\n",
      "Episode 90: Reward = 352.0, Avg10 = 378.40, BestAvg10 = 542.00\n",
      "Saved checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep90.pth\n",
      "Episode 91: Reward = 352.0, Avg10 = 391.60, BestAvg10 = 542.00\n",
      "Episode 91: Reward = 352.0, Avg10 = 391.60, BestAvg10 = 542.00\n",
      "Episode 92: Reward = 308.0, Avg10 = 360.80, BestAvg10 = 542.00\n",
      "Episode 92: Reward = 308.0, Avg10 = 360.80, BestAvg10 = 542.00\n",
      "Episode 93: Reward = 660.0, Avg10 = 400.40, BestAvg10 = 542.00\n",
      "Episode 93: Reward = 660.0, Avg10 = 400.40, BestAvg10 = 542.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_beamrider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mtrain_beamrider\u001b[39m\u001b[34m(episodes, log_interval, save_path)\u001b[39m\n\u001b[32m     39\u001b[39m state = np.stack(frame_stack, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     40\u001b[39m action, log_prob, value = agent.select_action(state)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m obs, reward, terminated, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m     43\u001b[39m frame_stack.append(preprocess(obs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedan\\.conda\\envs\\ai\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedan\\.conda\\envs\\ai\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedan\\.conda\\envs\\ai\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedan\\.conda\\envs\\ai\\Lib\\site-packages\\ale_py\\env.py:305\u001b[39m, in \u001b[36mAtariEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    303\u001b[39m reward = \u001b[32m0.0\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     reward += \u001b[38;5;28mself\u001b[39m.ale.act(action_idx, strength)\n\u001b[32m    307\u001b[39m is_terminal = \u001b[38;5;28mself\u001b[39m.ale.game_over(with_truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    308\u001b[39m is_truncated = \u001b[38;5;28mself\u001b[39m.ale.game_truncated()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_beamrider(episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Utility to find the most recent run directory in runs/\n",
    "def get_latest_run_dir(base_dir=\"runs\"):\n",
    "    if not os.path.exists(base_dir):\n",
    "        return None\n",
    "    subdirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not subdirs:\n",
    "        return None\n",
    "    # Sort by modified time (latest last)\n",
    "    subdirs.sort(key=os.path.getmtime)\n",
    "    return subdirs[-1]\n",
    "\n",
    "def continue_training_from_last_run(extra_episodes=50, checkpoint_name=None, log_interval=10):\n",
    "    latest_run = get_latest_run_dir(\"runs\")\n",
    "    if latest_run is None:\n",
    "        print(\"No previous runs found in 'runs/'. Start a new training run first.\")\n",
    "        return\n",
    "\n",
    "    if checkpoint_name is None:\n",
    "        checkpoint_path = os.path.join(latest_run, \"checkpoint_ep90.pth\")\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(latest_run, checkpoint_name)\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint {checkpoint_path} not found.\\nAvailable files: {os.listdir(latest_run)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "    # Recreate environment and agent\n",
    "    env = gym.make(\"BeamRiderNoFrameskip-v4\", render_mode=\"human\")\n",
    "    obs, _ = env.reset()\n",
    "    obs = preprocess(obs)\n",
    "    state_shape = (4, 84, 84)\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    agent = PPOAgent(state_shape, n_actions)\n",
    "    agent.policy.load_state_dict(ckpt[\"policy_state_dict\"])\n",
    "    agent.optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "\n",
    "    # Restore previous reward history and episode index if available\n",
    "    reward_history = ckpt.get(\"reward_history\", [])\n",
    "    start_episode = ckpt.get(\"episode\", len(reward_history))\n",
    "    best_avg_reward = ckpt.get(\"best_avg_reward\", -float(\"inf\"))\n",
    "\n",
    "    log_file_path = os.path.join(latest_run, \"training_log.csv\")\n",
    "    if not os.path.exists(log_file_path):\n",
    "        # If for some reason the CSV is missing, re-create with header\n",
    "        with open(log_file_path, \"w\") as f:\n",
    "            f.write(\"episode,total_reward,avg_reward_10,best_reward\\n\")\n",
    "\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(np.zeros((84, 84), dtype=np.uint8))\n",
    "\n",
    "    for ep_offset in range(extra_episodes):\n",
    "        ep = start_episode + ep_offset\n",
    "        obs, _ = env.reset()\n",
    "        frame_stack.extend([preprocess(obs)] * 4)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        memory = {\"states\": [], \"actions\": [], \"log_probs\": [], \"rewards\": [], \"dones\": [], \"returns\": []}\n",
    "\n",
    "        while not done:\n",
    "            state = np.stack(frame_stack, axis=0)\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            frame_stack.append(preprocess(obs))\n",
    "\n",
    "            memory[\"states\"].append(state)\n",
    "            memory[\"actions\"].append(action)\n",
    "            memory[\"log_probs\"].append(log_prob)\n",
    "            memory[\"rewards\"].append(reward)\n",
    "            memory[\"dones\"].append(done)\n",
    "            total_reward += reward\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = np.stack(frame_stack, axis=0)\n",
    "            _, next_value = agent.policy(torch.FloatTensor(state).unsqueeze(0))\n",
    "            next_value = next_value.item()\n",
    "\n",
    "        memory[\"returns\"] = agent.compute_returns(memory[\"rewards\"], memory[\"dones\"], next_value)\n",
    "        agent.update(memory)\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "        recent_rewards = reward_history[-10:] if len(reward_history) >= 10 else reward_history\n",
    "        avg_r_10 = float(np.mean(recent_rewards))\n",
    "\n",
    "        # Update best model if improved\n",
    "        is_best = avg_r_10 > best_avg_reward\n",
    "        if is_best:\n",
    "            best_avg_reward = avg_r_10\n",
    "            torch.save({\n",
    "                \"policy_state_dict\": agent.policy.state_dict(),\n",
    "                \"optimizer_state_dict\": agent.optimizer.state_dict(),\n",
    "                \"reward_history\": reward_history,\n",
    "                \"episode\": ep + 1,\n",
    "                \"best_avg_reward\": best_avg_reward\n",
    "            }, os.path.join(latest_run, \"best_model.pth\"))\n",
    "\n",
    "        # Append to CSV log\n",
    "        with open(log_file_path, \"a\") as f:\n",
    "            f.write(f\"{ep+1},{total_reward},{avg_r_10},{best_avg_reward}\\n\")\n",
    "\n",
    "        print(f\"[CONT] Episode {ep+1}: Reward = {total_reward:.1f}, Avg10 = {avg_r_10:.2f}, BestAvg10 = {best_avg_reward:.2f}\")\n",
    "\n",
    "        # Periodic checkpoint during continued training\n",
    "        if (ep + 1) % log_interval == 0:\n",
    "            ckpt_path = os.path.join(latest_run, f\"checkpoint_ep{ep+1}.pth\")\n",
    "            torch.save({\n",
    "                \"policy_state_dict\": agent.policy.state_dict(),\n",
    "                \"optimizer_state_dict\": agent.optimizer.state_dict(),\n",
    "                \"reward_history\": reward_history,\n",
    "                \"episode\": ep + 1,\n",
    "                \"best_avg_reward\": best_avg_reward\n",
    "            }, ckpt_path)\n",
    "            print(f\"Saved continued-training checkpoint to {ckpt_path}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Finished continuing training for {extra_episodes} episodes in run folder: {latest_run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e21691df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: runs\\beamrider_20251119_140617\\checkpoint_ep90.pth\n",
      "[CONT] Episode 91: Reward = 264.0, Avg10 = 382.80, BestAvg10 = 542.00\n",
      "[CONT] Episode 92: Reward = 396.0, Avg10 = 360.80, BestAvg10 = 542.00\n",
      "[CONT] Episode 93: Reward = 264.0, Avg10 = 360.80, BestAvg10 = 542.00\n",
      "[CONT] Episode 94: Reward = 220.0, Avg10 = 365.20, BestAvg10 = 542.00\n",
      "[CONT] Episode 95: Reward = 176.0, Avg10 = 343.20, BestAvg10 = 542.00\n",
      "[CONT] Episode 96: Reward = 572.0, Avg10 = 338.80, BestAvg10 = 542.00\n",
      "[CONT] Episode 97: Reward = 396.0, Avg10 = 360.80, BestAvg10 = 542.00\n",
      "[CONT] Episode 98: Reward = 220.0, Avg10 = 316.80, BestAvg10 = 542.00\n",
      "[CONT] Episode 99: Reward = 220.0, Avg10 = 308.00, BestAvg10 = 542.00\n",
      "[CONT] Episode 100: Reward = 220.0, Avg10 = 294.80, BestAvg10 = 542.00\n",
      "Saved continued-training checkpoint to runs\\beamrider_20251119_140617\\checkpoint_ep100.pth\n",
      "Finished continuing training for 10 episodes in run folder: runs\\beamrider_20251119_140617\n"
     ]
    }
   ],
   "source": [
    "continue_training_from_last_run(extra_episodes=10, log_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
